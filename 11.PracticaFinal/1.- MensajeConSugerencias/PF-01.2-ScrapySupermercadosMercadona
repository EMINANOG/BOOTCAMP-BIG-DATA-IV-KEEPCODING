{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PF-01.2-ScrapySupermercadosMercadona","provenance":[{"file_id":"1ZSUs_JzdX149mUeVxH1hhalveToYzoPd","timestamp":1583680603603},{"file_id":"14GtdbO7v6Ed72kmLJwHK9SWSWYwhgoCV","timestamp":1567429645401},{"file_id":"1VDcjf-Tk69ZuNbx0hJdh_H6ZyccoRzsb","timestamp":1530860075838}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Fsnh0LjlYg3g","colab_type":"text"},"source":["Usaremos scrapy para obtener las direcciones de todos los supermercados de Madrid. En concreto, Mercadona y Carrefour."]},{"cell_type":"markdown","metadata":{"id":"-uy4cNBcp9Of","colab_type":"text"},"source":["Si aparece el mensaje \"ReactorNotRestartable\", reiniciar entorno de ejecución y volver a lanzar."]},{"cell_type":"code","metadata":{"id":"K7iWVmA7ZwjU","colab_type":"code","outputId":"5f2e090e-80b3-4fc5-d530-cb6a0047b872","executionInfo":{"status":"ok","timestamp":1585784174693,"user_tz":-120,"elapsed":3745,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":474}},"source":["!pip install scrapy"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: scrapy in /usr/local/lib/python3.6/dist-packages (2.0.1)\n","Requirement already satisfied: service-identity>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (18.1.0)\n","Requirement already satisfied: cryptography>=2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.8)\n","Requirement already satisfied: Twisted>=17.9.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (20.3.0)\n","Requirement already satisfied: cssselect>=0.9.1 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.1.0)\n","Requirement already satisfied: PyDispatcher>=2.0.5 in /usr/local/lib/python3.6/dist-packages (from scrapy) (2.0.5)\n","Requirement already satisfied: pyOpenSSL>=16.2.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (19.1.0)\n","Requirement already satisfied: queuelib>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.0)\n","Requirement already satisfied: protego>=0.1.15 in /usr/local/lib/python3.6/dist-packages (from scrapy) (0.1.16)\n","Requirement already satisfied: w3lib>=1.17.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.21.0)\n","Requirement already satisfied: parsel>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (1.5.2)\n","Requirement already satisfied: zope.interface>=4.1.3 in /usr/local/lib/python3.6/dist-packages (from scrapy) (5.0.2)\n","Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.6/dist-packages (from scrapy) (4.2.6)\n","Requirement already satisfied: pyasn1 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.4.8)\n","Requirement already satisfied: attrs>=16.0.0 in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (19.3.0)\n","Requirement already satisfied: pyasn1-modules in /usr/local/lib/python3.6/dist-packages (from service-identity>=16.0.0->scrapy) (0.2.8)\n","Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.14.0)\n","Requirement already satisfied: six>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from cryptography>=2.0->scrapy) (1.12.0)\n","Requirement already satisfied: incremental>=16.10.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (17.5.0)\n","Requirement already satisfied: constantly>=15.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (15.1.0)\n","Requirement already satisfied: hyperlink>=17.1.1 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (19.0.0)\n","Requirement already satisfied: PyHamcrest!=1.10.0,>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (2.0.2)\n","Requirement already satisfied: Automat>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from Twisted>=17.9.0->scrapy) (20.2.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from zope.interface>=4.1.3->scrapy) (46.0.0)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography>=2.0->scrapy) (2.20)\n","Requirement already satisfied: idna>=2.5 in /usr/local/lib/python3.6/dist-packages (from hyperlink>=17.1.1->Twisted>=17.9.0->scrapy) (2.8)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"4epay2uzgQ7d","colab_type":"code","outputId":"75963c36-376a-4145-9bc4-d91e16d6c23a","executionInfo":{"status":"ok","timestamp":1585784174695,"user_tz":-120,"elapsed":3740,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["# Conecta con google drive\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2OACwfM9b4kg"},"source":["Obtendremos un csv de PAGINAS AMARILLAS tras realizar la búsqueda de supermercados Mercadona en Madrid."]},{"cell_type":"code","metadata":{"id":"83VWlZeUHPCt","colab_type":"code","colab":{}},"source":["import scrapy\n","import json\n","\n","class MercadonaSpider(scrapy.Spider):\n","    name = 'Mercadona'\n","    start_urls = ['https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/1?what=mercadona&where=madrid+capital&ub=false&aprob=0.0&nprob=1.0&qc=true']\n","\n","    def parse(self, response):\n","        # Aqui scrapeamos los datos y los imprimimos a un fichero   \n","        for article in response.css('div.listado-item'):\n","            nombre = article.css('div.box div.cabecera div.row a h2 ::text').extract_first().strip().replace(',', '').replace('.', '')\n","            direccion_parts = article.css('span.location ::text').extract()\n","              \n","            # Print a un fichero\n","            print(f\"{nombre}, {direccion_parts[0]} {direccion_parts[1]} {direccion_parts[2]}\", file=filep)\n","\n","        # Aqui hacemos crawling (con el follow) Ojo, esto crawlea TODO!\n","        for next_page in response.css('ul.pagination li:nth-last-child(2) a'):\n","            yield response.follow(next_page, self.parse)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T1eaKUdiTnzx","colab_type":"code","outputId":"49bea234-6f9c-4b7b-8136-395f45c2f6bb","executionInfo":{"status":"ok","timestamp":1585784178060,"user_tz":-120,"elapsed":7096,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["# Sacamos las direcciones de todos los Mercadonas de Madrid.\n","filep = open('/content/drive/My Drive/1.Practica-Final/1.- Mensaje con sugerencias/PaginasAmarillasSupermercadosMercadona.csv', 'w')\n","\n","from scrapy.crawler import CrawlerProcess\n","\n","process = CrawlerProcess({\n","    #'USER_AGENT': 'Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1)'\n","    'USER_AGENT': 'RVEGAS CRAWLER'\n","})\n","\n","process.crawl(MercadonaSpider)\n","process.start()\n","filep.close()"],"execution_count":4,"outputs":[{"output_type":"stream","text":["2020-04-01 23:36:14 [scrapy.utils.log] INFO: Scrapy 2.0.1 started (bot: scrapybot)\n","2020-04-01 23:36:14 [scrapy.utils.log] INFO: Versions: lxml 4.2.6.0, libxml2 2.9.8, cssselect 1.1.0, parsel 1.5.2, w3lib 1.21.0, Twisted 20.3.0, Python 3.6.9 (default, Nov  7 2019, 10:44:02) - [GCC 8.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1d  10 Sep 2019), cryptography 2.8, Platform Linux-4.14.137+-x86_64-with-Ubuntu-18.04-bionic\n","2020-04-01 23:36:14 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n","2020-04-01 23:36:14 [scrapy.crawler] INFO: Overridden settings:\n","{'USER_AGENT': 'RVEGAS CRAWLER'}\n","2020-04-01 23:36:14 [scrapy.extensions.telnet] INFO: Telnet Password: b64f0da4e6cf3992\n","2020-04-01 23:36:14 [scrapy.middleware] INFO: Enabled extensions:\n","['scrapy.extensions.corestats.CoreStats',\n"," 'scrapy.extensions.telnet.TelnetConsole',\n"," 'scrapy.extensions.memusage.MemoryUsage',\n"," 'scrapy.extensions.logstats.LogStats']\n","2020-04-01 23:36:14 [scrapy.middleware] INFO: Enabled downloader middlewares:\n","['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n"," 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n"," 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n"," 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n"," 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n"," 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n"," 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n"," 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n"," 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n"," 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n","2020-04-01 23:36:14 [scrapy.middleware] INFO: Enabled spider middlewares:\n","['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n"," 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n"," 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n"," 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n"," 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n","2020-04-01 23:36:14 [scrapy.middleware] INFO: Enabled item pipelines:\n","[]\n","2020-04-01 23:36:14 [scrapy.core.engine] INFO: Spider opened\n","2020-04-01 23:36:14 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n","2020-04-01 23:36:14 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n","2020-04-01 23:36:15 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/1?what=mercadona&where=madrid+capital&ub=false&aprob=0.0&nprob=1.0&qc=true> (referer: None)\n","2020-04-01 23:36:16 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/2?what=mercadona&where=madrid+capital&aprob=0.0&nprob=1.0> (referer: https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/1?what=mercadona&where=madrid+capital&ub=false&aprob=0.0&nprob=1.0&qc=true)\n","2020-04-01 23:36:17 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/3?what=mercadona&where=madrid+capital&aprob=0.0&nprob=1.0> (referer: https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/2?what=mercadona&where=madrid+capital&aprob=0.0&nprob=1.0)\n","2020-04-01 23:36:17 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://www.paginasamarillas.es/search/all-ac/all-ma/madrid/all-is/madrid-capital/all-ba/all-pu/all-nc/2?what=mercadona&where=madrid+capital&aprob=0.0&nprob=1.0> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n","2020-04-01 23:36:17 [scrapy.core.engine] INFO: Closing spider (finished)\n","2020-04-01 23:36:17 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n","{'downloader/request_bytes': 1713,\n"," 'downloader/request_count': 3,\n"," 'downloader/request_method_count/GET': 3,\n"," 'downloader/response_bytes': 66765,\n"," 'downloader/response_count': 3,\n"," 'downloader/response_status_count/200': 3,\n"," 'dupefilter/filtered': 1,\n"," 'elapsed_time_seconds': 3.035551,\n"," 'finish_reason': 'finished',\n"," 'finish_time': datetime.datetime(2020, 4, 1, 23, 36, 17, 641241),\n"," 'log_count/DEBUG': 4,\n"," 'log_count/INFO': 10,\n"," 'memusage/max': 181637120,\n"," 'memusage/startup': 181637120,\n"," 'request_depth_max': 3,\n"," 'response_received_count': 3,\n"," 'scheduler/dequeued': 3,\n"," 'scheduler/dequeued/memory': 3,\n"," 'scheduler/enqueued': 3,\n"," 'scheduler/enqueued/memory': 3,\n"," 'start_time': datetime.datetime(2020, 4, 1, 23, 36, 14, 605690)}\n","2020-04-01 23:36:17 [scrapy.core.engine] INFO: Spider closed (finished)\n"],"name":"stderr"}]}]}