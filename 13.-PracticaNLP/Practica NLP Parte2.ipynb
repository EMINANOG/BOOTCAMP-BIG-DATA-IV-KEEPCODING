{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practica NLP Parte2.ipynb","provenance":[{"file_id":"1HhslUwHdkzH6ijWI26a7AkovMUfM5RU6","timestamp":1582365727099},{"file_id":"1KYAx17NDJSgA1IETdrjbRdrW0Bw4Hsb3","timestamp":1530783736511}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"pRJ7fiaDYMqo","colab_type":"text"},"source":["#2. Hacer Analysis de los tweets del segundo dataset. Que temas aparecen? Como se representan estos temas? De que hablan unos y otros?\n"]},{"cell_type":"markdown","metadata":{"id":"8pjmpz7yNgcW","colab_type":"text"},"source":["Tenemos un caso aquí de NLP sin etiquetas/lables. Tenemos que usar Aprendizaje no supervisado, modelo Topic Modeling (genera nclusters, cada uno de ellos es una temática, representando tweets con topics similares y recomendarlos, gracias a que un documento esta formado por n-topics, y cada topic esta formado por una distribución de palabras).\n","Corpus de documentos se pasa por el algoritmo LDA, generará temáticas topics clusters de palabras, y ver e ncada documento la distrub de topics "]},{"cell_type":"code","metadata":{"id":"_noskQJ_ZxAD","colab_type":"code","outputId":"60241c6c-b54d-4e36-9b1b-3a56527872e3","executionInfo":{"status":"ok","timestamp":1585419248742,"user_tz":-60,"elapsed":22777,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["#Librerías\n","!pip install gensim\n","!pip install pyLDAvis\n","!pip install spacy\n","!python -m spacy download en_core_web_sm\n","!pip install stop_words"],"execution_count":1,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n","Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.2)\n","Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.10.0)\n","Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n","Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n","Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.12.27)\n","Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.21.0)\n","Requirement already satisfied: google-cloud-storage in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.18.1)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.9.5)\n","Requirement already satisfied: botocore<1.16.0,>=1.15.27 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.15.27)\n","Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.8)\n","Requirement already satisfied: google-cloud-core<2.0dev,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.0.3)\n","Requirement already satisfied: google-auth>=1.2.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (1.7.2)\n","Requirement already satisfied: google-resumable-media<0.5.0dev,>=0.3.1 in /usr/local/lib/python3.6/dist-packages (from google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.1)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n","Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.16.0,>=1.15.27->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n","Requirement already satisfied: google-api-core<2.0.0dev,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.16.0)\n","Requirement already satisfied: rsa<4.1,>=3.1.4 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.2.8)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (46.0.0)\n","Requirement already satisfied: cachetools<3.2,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.1.1)\n","Requirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (3.10.0)\n","Requirement already satisfied: googleapis-common-protos<2.0dev,>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (1.51.0)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.6/dist-packages (from google-api-core<2.0.0dev,>=1.14.0->google-cloud-core<2.0dev,>=1.0.0->google-cloud-storage->smart-open>=1.2.1->gensim) (2018.9)\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<4.1,>=3.1.4->google-auth>=1.2.0->google-cloud-storage->smart-open>=1.2.1->gensim) (0.4.8)\n","Collecting pyLDAvis\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/3a/af82e070a8a96e13217c8f362f9a73e82d61ac8fff3a2561946a97f96266/pyLDAvis-2.1.2.tar.gz (1.6MB)\n","\u001b[K     |████████████████████████████████| 1.6MB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: wheel>=0.23.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.34.2)\n","Requirement already satisfied: numpy>=1.9.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.18.2)\n","Requirement already satisfied: scipy>=0.18.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (1.4.1)\n","Requirement already satisfied: pandas>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.25.3)\n","Requirement already satisfied: joblib>=0.8.4 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.14.1)\n","Requirement already satisfied: jinja2>=2.7.2 in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.11.1)\n","Requirement already satisfied: numexpr in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (2.7.1)\n","Requirement already satisfied: pytest in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (3.6.4)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyLDAvis) (0.16.0)\n","Collecting funcy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ce/4b/6ffa76544e46614123de31574ad95758c421aae391a1764921b8a81e1eae/funcy-1.14.tar.gz (548kB)\n","\u001b[K     |████████████████████████████████| 552kB 21.9MB/s \n","\u001b[?25hRequirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2018.9)\n","Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.17.0->pyLDAvis) (2.8.1)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.7.2->pyLDAvis) (1.1.1)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (46.0.0)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.12.0)\n","Requirement already satisfied: atomicwrites>=1.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.3.0)\n","Requirement already satisfied: more-itertools>=4.0.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (8.2.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (19.3.0)\n","Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (1.8.1)\n","Requirement already satisfied: pluggy<0.8,>=0.5 in /usr/local/lib/python3.6/dist-packages (from pytest->pyLDAvis) (0.7.1)\n","Building wheels for collected packages: pyLDAvis, funcy\n","  Building wheel for pyLDAvis (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyLDAvis: filename=pyLDAvis-2.1.2-py2.py3-none-any.whl size=97711 sha256=a3d828b7e349998eb07621895b550bc82df027b279e0b2d88feff65197d1d71c\n","  Stored in directory: /root/.cache/pip/wheels/98/71/24/513a99e58bb6b8465bae4d2d5e9dba8f0bef8179e3051ac414\n","  Building wheel for funcy (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for funcy: filename=funcy-1.14-py2.py3-none-any.whl size=32042 sha256=237378cdab8905fb647c34a037d3eb9127e6b5a8f8918634eaf9ee95f6e718b4\n","  Stored in directory: /root/.cache/pip/wheels/20/5a/d8/1d875df03deae6f178dfdf70238cca33f948ef8a6f5209f2eb\n","Successfully built pyLDAvis funcy\n","Installing collected packages: funcy, pyLDAvis\n","Successfully installed funcy-1.14 pyLDAvis-2.1.2\n","Requirement already satisfied: spacy in /usr/local/lib/python3.6/dist-packages (2.2.4)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.38.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.6.0)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.2)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.21.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.0)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (46.0.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (1.5.0)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2019.11.28)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.1.0)\n","Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.3)\n","Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n","Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.38.0)\n","Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.2)\n","Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.18.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (46.0.0)\n","Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.21.0)\n","Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.6.0)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.2)\n","Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.5.0)\n","Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2019.11.28)\n","Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.1.0)\n","\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n","You can now load the model via spacy.load('en_core_web_sm')\n","Collecting stop_words\n","  Downloading https://files.pythonhosted.org/packages/1c/cb/d58290804b7a4c5daa42abbbe2a93c477ae53e45541b1825e86f0dfaaf63/stop-words-2018.7.23.tar.gz\n","Building wheels for collected packages: stop-words\n","  Building wheel for stop-words (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for stop-words: filename=stop_words-2018.7.23-cp36-none-any.whl size=32917 sha256=647c621f13fa1819e25af62334d812521bd3be274968e666496c6a4d7c84e31a\n","  Stored in directory: /root/.cache/pip/wheels/75/37/6a/2b295e03bd07290f0da95c3adb9a74ba95fbc333aa8b0c7c78\n","Successfully built stop-words\n","Installing collected packages: stop-words\n","Successfully installed stop-words-2018.7.23\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"MLxHtBgwNgcZ","colab_type":"code","colab":{}},"source":["# Librerías\n","import gensim\n","from os.path import join\n","import csv\n","import spacy\n","from stop_words import get_stop_words\n","from string import punctuation\n","import re\n","import numpy as np\n","import pyLDAvis.gensim\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import io\n","\n","import warnings\n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k0Fqi4_32g0c","colab_type":"text"},"source":["Preprocesado de datos."]},{"cell_type":"code","metadata":{"id":"f_JmG4m_Ngcc","colab_type":"code","colab":{}},"source":["# Obtenemos una lista de 'stop words', palabras como artículos o adverbios que no son topics\n","stop_words = get_stop_words('en') + list(punctuation) + [' ']\n","nlp = spacy.load('en_core_web_sm')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ye1KMVAof8jK","colab_type":"code","outputId":"93d690f5-258f-4677-a73d-f1403f8e280a","executionInfo":{"status":"ok","timestamp":1585419311193,"user_tz":-60,"elapsed":705,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":54}},"source":["import json\n","\n","import numpy as np \n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Crea un diccionario con campos de quién ha escrito el tweet (Obama/Trump) y su contenido\n","dataS = json.loads((open('/content/drive/My Drive/dataset_2.json').read()))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"7GIutMpvoOxt","colab_type":"code","colab":{}},"source":["# Convertir diccionario en dataframe de 5889 resgistros\n","df = pd.DataFrame([key for key in dataS.keys()], columns=['NumID'])\n","df['label'] = [value['label'] for value in dataS.values()]\n","df['tweet'] = [value['tweet'] for value in dataS.values()]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"u-YxpIJ2pDIe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":195},"outputId":"d4d21f62-0b76-4b51-adec-08219712d066","executionInfo":{"status":"ok","timestamp":1585419315645,"user_tz":-60,"elapsed":985,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}}},"source":["df.head(5)  # NumID, label(OBAMA/TRUMP), tweet(Contenido)"],"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NumID</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>OBAMA</td>\n","      <td>“Low plastic stool, cheap but delicious noodle...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>OBAMA</td>\n","      <td>“Low plastic stool, cheap but delicious noodle...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>OBAMA</td>\n","      <td>This National Gun Violence Awareness Day, show...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>OBAMA</td>\n","      <td>We can never truly repay the debt we owe our f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>OBAMA</td>\n","      <td>This Center is for the leaders of tomorrow who...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["  NumID  label                                              tweet\n","0     0  OBAMA  “Low plastic stool, cheap but delicious noodle...\n","1     1  OBAMA  “Low plastic stool, cheap but delicious noodle...\n","2     2  OBAMA  This National Gun Violence Awareness Day, show...\n","3     3  OBAMA  We can never truly repay the debt we owe our f...\n","4     4  OBAMA  This Center is for the leaders of tomorrow who..."]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"code","metadata":{"id":"GVefw_BDq5VY","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":402},"outputId":"a4e6fdff-5c43-47a0-c69a-8e190a88e905","executionInfo":{"status":"ok","timestamp":1585419318682,"user_tz":-60,"elapsed":625,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}}},"source":["#Eliminamos la primera fila, que se ve que es un duplicado\n","df.drop([0],axis=0)"],"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>NumID</th>\n","      <th>label</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>OBAMA</td>\n","      <td>“Low plastic stool, cheap but delicious noodle...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>OBAMA</td>\n","      <td>This National Gun Violence Awareness Day, show...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>OBAMA</td>\n","      <td>We can never truly repay the debt we owe our f...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>OBAMA</td>\n","      <td>This Center is for the leaders of tomorrow who...</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>OBAMA</td>\n","      <td>Happy Mother’s Day to every mom out there, esp...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>5884</th>\n","      <td>5884</td>\n","      <td>TRUMP</td>\n","      <td>...vast sums of money to NATO &amp;amp; the United...</td>\n","    </tr>\n","    <tr>\n","      <th>5885</th>\n","      <td>5885</td>\n","      <td>TRUMP</td>\n","      <td>Despite what you have heard from the FAKE NEWS...</td>\n","    </tr>\n","    <tr>\n","      <th>5886</th>\n","      <td>5886</td>\n","      <td>TRUMP</td>\n","      <td>Great meeting with the @RepublicanStudy Commit...</td>\n","    </tr>\n","    <tr>\n","      <th>5887</th>\n","      <td>5887</td>\n","      <td>TRUMP</td>\n","      <td>\"The President Changed. So Has Small Businesse...</td>\n","    </tr>\n","    <tr>\n","      <th>5888</th>\n","      <td>5888</td>\n","      <td>TRUMP</td>\n","      <td>North Korea is behaving very badly. They have ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>5888 rows × 3 columns</p>\n","</div>"],"text/plain":["     NumID  label                                              tweet\n","1        1  OBAMA  “Low plastic stool, cheap but delicious noodle...\n","2        2  OBAMA  This National Gun Violence Awareness Day, show...\n","3        3  OBAMA  We can never truly repay the debt we owe our f...\n","4        4  OBAMA  This Center is for the leaders of tomorrow who...\n","5        5  OBAMA  Happy Mother’s Day to every mom out there, esp...\n","...    ...    ...                                                ...\n","5884  5884  TRUMP  ...vast sums of money to NATO &amp; the United...\n","5885  5885  TRUMP  Despite what you have heard from the FAKE NEWS...\n","5886  5886  TRUMP  Great meeting with the @RepublicanStudy Commit...\n","5887  5887  TRUMP  \"The President Changed. So Has Small Businesse...\n","5888  5888  TRUMP  North Korea is behaving very badly. They have ...\n","\n","[5888 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":8}]},{"cell_type":"code","metadata":{"id":"mbg57gtPrHZG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":218},"outputId":"256db5e7-9a3b-48f0-896a-111ea175a257","executionInfo":{"status":"ok","timestamp":1585419550931,"user_tz":-60,"elapsed":653,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ggppi28LwGqiXXUwE6-4pIsOWemXI5fI8v7y1JDrQ=s64","userId":"09902616180052958600"}}},"source":["# Se ve que no hay nulos\n","df['tweet']"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"text/plain":["0       “Low plastic stool, cheap but delicious noodle...\n","1       “Low plastic stool, cheap but delicious noodle...\n","2       This National Gun Violence Awareness Day, show...\n","3       We can never truly repay the debt we owe our f...\n","4       This Center is for the leaders of tomorrow who...\n","                              ...                        \n","5884    ...vast sums of money to NATO &amp; the United...\n","5885    Despite what you have heard from the FAKE NEWS...\n","5886    Great meeting with the @RepublicanStudy Commit...\n","5887    \"The President Changed. So Has Small Businesse...\n","5888    North Korea is behaving very badly. They have ...\n","Name: tweet, Length: 5889, dtype: object"]},"metadata":{"tags":[]},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"aoSkOA2NqDy0","colab_type":"text"},"source":["### Preparar Dataset"]},{"cell_type":"code","metadata":{"id":"-7wBBk6nNgck","colab_type":"code","colab":{}},"source":["# Nos quedamos con las palabras de cada tweet\n","# si no está en la lista de stop_words presentada arriba no se tiene en cuenta\n","documents = []               # inicializa array\n","for row in df.iterrows():    # para cada registro \n","    _, info = row            # info de cada registro  \n","    _, q, a = info           # nos quedamos con label y con el contedio del tweet\n","    q = re.sub(r'\\n', '', q) # nos quedamos con las palabras\n","    a = re.sub(r'\\n', '', a) #\n","    q = [t.text for t in nlp(q.rstrip(), disable=['parser', 'tagger', 'ner']) if t.text not in stop_words]\n","    a = [t.text for t in nlp(a.rstrip(), disable=['parser', 'tagger', 'ner']) if t.text not in stop_words]\n","    documents.append(q) # junta todo en un mismo corpus\n","    documents.append(a)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GJI1zFRMNgcp","colab_type":"code","colab":{}},"source":["vocab = set([t for doc in documents for t in doc]) # conjunto de palabras(con lo que las palabras serán únicas)\n","w2id = {k:i for  i,k in enumerate(vocab)}          # crea un diccionario de las palabras de vocab \n","id2w = {i:k for k, i in w2id.items()}              # lo convierte en tokens\n","print('{} unique tokens'.format(len(w2id)))        #  muestra el número de tokens únicos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVH8PD9rNgc7","colab_type":"code","colab":{}},"source":["# Gensim es una biblioteca de código abierto para el modelado de temas no supervisados ​​y el procesamiento del \n","# lenguaje natural, que utiliza el aprendizaje automático moderno de estadística.\n","from gensim.corpora import Dictionary"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"3LtUite3Ngc-","colab_type":"code","colab":{}},"source":["#Crea un diccionario con las palabras extraídas\n","gensim_dict = Dictionary(documents)\n","len(gensim_dict) # número de elementos del diccionario"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkhPYIIaNgdD","colab_type":"code","colab":{}},"source":["# corpus : colección de documentos\n","# doc2bow crea una bolsa de palabras (una lista con un núm id de la palabra y el número de veces que aparece)\n","corpus = [gensim_dict.doc2bow(doc) for doc in documents]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VOH850Lg25_h","colab_type":"text"},"source":["Implementación usando Gensim. Crearemos varios modelos, y veremos con cuál nos quedamos."]},{"cell_type":"code","metadata":{"id":"90vTaSSCNgdG","colab_type":"code","colab":{}},"source":["#build a model\n","from gensim import corpora, models, similarities"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lq6LG1vdNgdI","colab_type":"code","colab":{}},"source":["# 1.-Modelo \"lda_big\"\n","# Entrena modelo LDA, pasándole el corpus y el diccionario calculado arriba\n","# Busca 50 topics como random para empezar a priori y sin explorar. Es necesaria la coherencia para que no se \n","# solapen unos con otros.\n","lda_big = models.LdaModel(corpus, id2word=gensim_dict, num_topics=50, iterations=5, passes=20, alpha='auto')\n","print(lda_big.log_perplexity(corpus)) # muestra por pantalla la perplejidad -8.403881139627035\n","print(lda_big.bound(corpus))\n","vis = pyLDAvis.gensim.prepare(lda_big, corpus, gensim_dict)\n","pyLDAvis.display(vis)\n","#No parece que sea un modelo recomendable, pues muchos topics se solapan unos a otros"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"yA_Mx4GNNgdM","colab_type":"code","colab":{}},"source":["# 2.-Modelo \"lda_mid\"\n","# Entrena modelo LDA, pasándole el corpus y el diccionario calculado arriba.\n","# Busca 20 topics.\n","lda_mid = models.LdaModel(corpus, id2word=gensim_dict, num_topics=20, iterations=5, passes=20, alpha='auto')\n","print(lda_mid.bound(corpus))\n","vis = pyLDAvis.gensim.prepare(lda_mid, corpus, gensim_dict)\n","pyLDAvis.display(vis)\n","# Mejora respecto del modelo anterio, pero todavía muchos tópics se solapan"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"JomVf_UhNgdV","colab_type":"code","colab":{}},"source":["# 3.-Modelo \"lda_small\"\n","# Entrena modelo LDA, pasándole el corpus y el diccionario calculado arriba.\n","# Busca 10 topics.\n","lda_small = models.LdaModel(corpus, id2word=gensim_dict, num_topics=10, iterations=5, passes=20, alpha='auto')\n","print(lda_small.bound(corpus))\n","vis = pyLDAvis.gensim.prepare(lda_small, corpus, gensim_dict)\n","pyLDAvis.display(vis)\n","# Sigue mejorando, pero quedan solapamientos"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CG_UAxBANgdc","colab_type":"code","colab":{}},"source":["# 4.-Modelo \"lda_tiny\"\n","# Entrena modelo LDA, pasándole el corpus y el diccionario calculado arriba.\n","# Busca 5 topics.\n","lda_tiny = models.LdaModel(corpus, id2word=gensim_dict, num_topics=5, iterations=5, passes=20, alpha='auto')\n","print(lda_tiny.bound(corpus))\n","vis = pyLDAvis.gensim.prepare(lda_tiny, corpus, gensim_dict)\n","pyLDAvis.display(vis)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZkxX_CYJ3c-D","colab_type":"text"},"source":["Análisis de los modelos."]},{"cell_type":"code","metadata":{"id":"wavHWOCpNgdf","colab_type":"code","colab":{}},"source":["# Buscaremos el número de topics que más coherencia nos de\n","from gensim.models import CoherenceModel\n","def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n","    \"\"\"\n","    Compute c_v coherence for various number of topics\n","\n","    Parameters:\n","    ----------\n","    dictionary : Gensim dictionary\n","    corpus : Gensim corpus\n","    texts : List of input texts\n","    limit : Max num of topics\n","\n","    Returns:\n","    -------\n","    model_list : List of LDA topic models\n","    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n","    \"\"\"\n","    \n","    coherence_values = []\n","    model_list = []\n","    for num_topics in range(start, limit, step):\n","        model = models.LdaModel(corpus, id2word=dictionary, num_topics=num_topics, iterations=5, passes=20, alpha='auto')\n","        model_list.append(model)\n","        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n","        coherence_values.append(coherencemodel.get_coherence())\n","\n","    return model_list, coherence_values"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7IbMSU5PNgdh","colab_type":"code","colab":{}},"source":["#Representamos gráficamente para no tirarnos días\n","limit=40; start=2; step=2; # buscará topics de dos en dos, desde 2 hasta 40\n","model_list, coherence_values = compute_coherence_values(dictionary=gensim_dict, corpus=corpus, texts=documents, start=start, limit=limit, step=step)\n","x = range(start, limit, step)\n","plt.plot(x, coherence_values)\n","plt.xlabel(\"Num Topics\")           # eje x, número de topics\n","plt.ylabel(\"Coherence score\")      # eje y, coherencia\n","plt.legend((\"coherence_values\"), loc='best')\n","plt.show()\n","\n","# Según el gráfico, el mejor número de topics es 3-4 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kbOaws34sOWy","colab_type":"code","colab":{}},"source":["# Modelo con mejor número de topics\n","# Entrena modelo LDA, pasándole el corpus y el diccionario calculado arriba.\n","# Busca 3 topics.\n","lda_tiny = models.LdaModel(corpus, id2word=gensim_dict, num_topics=3, iterations=5, passes=20, alpha='auto')\n","print(lda_tiny.bound(corpus))\n","vis = pyLDAvis.gensim.prepare(lda_tiny, corpus, gensim_dict)\n","pyLDAvis.display(vis)\n","\n","# Si vamos a los topics, y marcamos sobre ellos, muestra los términos más relevantes de ése topic.\n","# Para analizar los temas que genera el algoritmo veamos uno a uno :\n","# Num1 : Algo confuso...se distinguen \"Fake\" y \"News\" \n","# Num2 : Aquí destaca \"Climate\", \"Health\", \"Clinton\"\n","# Num3 : Éste está muy claro \"Minister\", \"Prime\", \"attack\", \"terrorist\", \"condolences\", \"London\"...se habla de los \n","#        ataques terroristas en Londres\n"],"execution_count":0,"outputs":[]}]}