{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Practica NLP Parte3-Trump.ipynb","provenance":[{"file_id":"1SMxqOADfGJfAg7kUFLQ_6EOZVEOADrvo","timestamp":1582393085463}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"pRJ7fiaDYMqo","colab_type":"text"},"source":["##3. Escoged a uno de los dos presidentes, y escribid tweets como ellos, usando un Modelo Generativo. Donald Trump.\n"]},{"cell_type":"code","metadata":{"id":"9KTjchCwMzzD","colab_type":"code","colab":{}},"source":["#Importar librerías\n","\n","import spacy\n","import numpy as np\n","\n","import pickle\n","import json\n","import os\n","import csv\n","import pprint as pp\n","\n","import seaborn as sns\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","\n","from random import shuffle, choice, sample\n","\n","from sklearn.model_selection import StratifiedShuffleSplit\n","\n","from copy import copy\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","import pylab as pl\n","from IPython import display\n","\n","sns.set(color_codes=True)\n","\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","\n","%matplotlib inline\n","%load_ext autoreload\n","%autoreload 2\n","%matplotlib notebook"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"GfqsamXkMzzG","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":96},"outputId":"9c85b251-15ba-424b-d3e9-67b6aab573fe","executionInfo":{"status":"ok","timestamp":1582497933034,"user_tz":-60,"elapsed":9928,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}}},"source":["# Importar librerías\n","\n","from keras.models import Model, Sequential\n","from keras.layers import Input, CuDNNLSTM, Dense, LSTM\n","from keras.layers import Bidirectional\n","from keras.layers import Embedding\n","from keras.layers import Dot, Concatenate, Flatten, Permute, Multiply, dot, concatenate\n","from keras.layers import TimeDistributed\n","from keras.layers import Activation\n","from keras.preprocessing import sequence\n","from keras.callbacks import Callback\n","from keras.optimizers import SGD\n","from keras.models import load_model\n","\n","from keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["<p style=\"color: red;\">\n","The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n","We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n","or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n","<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"id":"JgQqy9l6HPNu","colab_type":"text"},"source":["## Preprocess"]},{"cell_type":"code","metadata":{"id":"y-QjTcfqHonk","colab_type":"code","outputId":"f9c06611-165a-4f73-d758-e14829ca0686","executionInfo":{"status":"ok","timestamp":1582497957148,"user_tz":-60,"elapsed":34037,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":121}},"source":["from google.colab import files\n","import io\n","import json\n","\n","import numpy as np \n","import pandas as pd\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","#Crea un diccionario con los tweets de Trump. \n","dataS = json.loads((open('/content/drive/My Drive/BootCamp - BigDataIV - NLP/data_practica_realDonaldTrump.json').read()))"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"qirYgHZjJg56","colab_type":"code","colab":{}},"source":["# Convertir diccionario en dataframe de registros. \n","df = pd.DataFrame([[key, dataS[key]] for key in dataS.keys()], columns=['Id', 'TrumpTweet'])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wZb6kzuJJrdA","colab_type":"code","outputId":"a0523016-cdf1-4243-e731-a6c8b365f286","executionInfo":{"status":"ok","timestamp":1582497957149,"user_tz":-60,"elapsed":34032,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":402}},"source":["df"],"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Id</th>\n","      <th>TrumpTweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>My thoughts and prayers are with the families ...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>My thoughts and prayers are with the families ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>I am heading for Canada and the G-7 for talks ...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>Congratulations to the Washington Capitals on ...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>Looking forward to straightening out unfair Tr...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>2990</th>\n","      <td>2990</td>\n","      <td>...vast sums of money to NATO &amp;amp; the United...</td>\n","    </tr>\n","    <tr>\n","      <th>2991</th>\n","      <td>2991</td>\n","      <td>Despite what you have heard from the FAKE NEWS...</td>\n","    </tr>\n","    <tr>\n","      <th>2992</th>\n","      <td>2992</td>\n","      <td>Great meeting with the @RepublicanStudy Commit...</td>\n","    </tr>\n","    <tr>\n","      <th>2993</th>\n","      <td>2993</td>\n","      <td>\"The President Changed. So Has Small Businesse...</td>\n","    </tr>\n","    <tr>\n","      <th>2994</th>\n","      <td>2994</td>\n","      <td>North Korea is behaving very badly. They have ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>2995 rows × 2 columns</p>\n","</div>"],"text/plain":["        Id                                         TrumpTweet\n","0        0  My thoughts and prayers are with the families ...\n","1        1  My thoughts and prayers are with the families ...\n","2        2  I am heading for Canada and the G-7 for talks ...\n","3        3  Congratulations to the Washington Capitals on ...\n","4        4  Looking forward to straightening out unfair Tr...\n","...    ...                                                ...\n","2990  2990  ...vast sums of money to NATO &amp; the United...\n","2991  2991  Despite what you have heard from the FAKE NEWS...\n","2992  2992  Great meeting with the @RepublicanStudy Commit...\n","2993  2993  \"The President Changed. So Has Small Businesse...\n","2994  2994  North Korea is behaving very badly. They have ...\n","\n","[2995 rows x 2 columns]"]},"metadata":{"tags":[]},"execution_count":5}]},{"cell_type":"code","metadata":{"id":"1KSqyeCm9usk","colab_type":"code","colab":{}},"source":["# Crea una lista de 2994 registros con el número de índice y el texto del tweet\n","dataset = list()\n","for index, row in df.iterrows():  # para cada registro\n","    if index > 0:\n","        sentence = row[1]         # toms el texto de cada registro\n","        dataset.append(sentence)  # añade el texto del tweet al dataset"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jW5c-nE6MzzP","colab_type":"code","colab":{}},"source":["tokenized = [list(x) for x in dataset] # Convierte cada clase en tokens por caracteres"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KkB5WUlbMzzS","colab_type":"code","colab":{}},"source":["init_chars = [x[:5] for x in tokenized] # me quedo con los 4 caracteres iniciales\n","\n","for i in range(len(init_chars)): # genera lista de tokens de los cinco primeros de cada frase\n","    tmp = init_chars[i]\n","    tmp.insert(0, '<SOS>') # añadiendo el token especial SOS\n","    init_chars[i] = tmp[:5]"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"_zCvGJnqMzzT","colab_type":"code","outputId":"4f6c3ffc-4ec3-4805-81ba-8dc818423c95","executionInfo":{"status":"ok","timestamp":1582497957151,"user_tz":-60,"elapsed":34024,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["maxlen = max([len(x) for x in tokenized])  # max longitud de cada clase tokenizada por caracteres       \n","avglen = sum([len(x) for x in tokenized]) / len(tokenized) # media de caracteres por frase en los tweets\n","maxlen, avglen"],"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(320, 160.39111556446227)"]},"metadata":{"tags":[]},"execution_count":9}]},{"cell_type":"code","metadata":{"id":"gbgDlWJwtx6R","colab_type":"code","outputId":"993fecc3-3fae-4583-99d5-0c5af80f9bf1","executionInfo":{"status":"ok","timestamp":1582497957151,"user_tz":-60,"elapsed":34020,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["total_tokens = [t for s in dataset for t in s] # total de caracteres\n","len(total_tokens)"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["480211"]},"metadata":{"tags":[]},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"a0z_bbdRMzzX","colab_type":"code","outputId":"311c3516-ceda-4660-df73-a70ba1bef7ec","executionInfo":{"status":"ok","timestamp":1582497957151,"user_tz":-60,"elapsed":34016,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["from collections import Counter\n","vocab_counter = Counter(total_tokens) # Cuenta el número de veces que aparece cada carácter\n","vocab = [w for w, v in vocab_counter.items() if v>2] # filtrado de caracteres que minimo hayan aparecido 2 veces\n","vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + vocab # añade al vocabulario los tokens especiales\n","nb_vocab = len(vocab)\n","nb_vocab"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/plain":["111"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"mgV7eHVfMzzc","colab_type":"code","colab":{}},"source":["c2id = {k: i for i, k in enumerate(vocab)} # asigna a cada token un número\n","id2c = {i: k for k, i in c2id.items()}     # crea un diccionario"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xG6wbq1oMzzh","colab_type":"code","outputId":"bbc6f3c3-4ed8-471b-fbbf-3187906314fc","executionInfo":{"status":"ok","timestamp":1582497958480,"user_tz":-60,"elapsed":35338,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["maxlen = 5\n","step = 1\n","data_train = []                # dataset al que iremos añadiendo\n","\n","for x in tokenized:            # x es una lista con mi frase \"tokenizada\"\n","    x.insert(0, '<SOS>')       # se le inserta al principio de cada frase SOS\n","    x.append('<EOS>')          # se le inserta al final de cada frase EOS\n","    for i in range(0, len(x)-maxlen, step): # longitud de cada frase tokenizada-5\n","        # se añade a la lista un conjunto de 1-5 primeros caracteres de cada frase, luego 2-6, 3-7...\n","        data_train.append((x[i:i+maxlen], x[i+maxlen])) \n","len(data_train) # esta lista tiene 471.229"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["471229"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"fBquffzJMzzy","colab_type":"code","colab":{}},"source":["SAMPLE_EVERY = 4"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4tdteb0lHTNN","colab_type":"text"},"source":["Callbacks : una devolución de llamada o retrollamada es una función \"A\" que se usa como argumento de otra función \"B\". Cuando se llama a \"B\", ésta ejecuta \"A\". Para conseguirlo, usualmente lo que se pasa a \"B\" es el puntero a \"A\"."]},{"cell_type":"code","metadata":{"id":"gq0lOMoyMzz0","colab_type":"code","colab":{}},"source":["# función que ayuda a que las predicciones de los caracteres ganen diversidad \n","def sample_pred(preds, temperature=1.0):\n","    # helper function to sample an index from a probability array\n","    preds = np.asarray(preds).astype('float64')\n","    preds = np.log(preds) / temperature\n","    exp_preds = np.exp(preds)\n","    preds = exp_preds / np.sum(exp_preds)\n","    probas = np.random.multinomial(1, preds, 1)\n","    return np.argmax(probas)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVMl3_E0Mzz-","colab_type":"code","colab":{}},"source":["class Sampletest(Callback):\n","# cuando termine la epoch de entreno, me da una muestra de qué texto está aprendiendo, y el progreso a nivel de \n","# frases generadas\n","    def on_epoch_end(self, epoch, logs):  \n","        if epoch % SAMPLE_EVERY == 0  and epoch>0:\n","            data_test = []\n","            nb_samples = 1\n","            \n","            params = {\n","                'maxlen': maxlen,\n","                'vocab': len(vocab),\n","                'use_embeddings': True\n","                }\n","            for _ in range(nb_samples):\n","                data_test = choice(init_chars)\n","                for diversity in [0.2, 0.6, 1.2]:\n","                    print('----- diversity:', diversity)\n","                    sentence = copy(data_test)\n","                    generated = copy(data_test)\n","                    for i in range(len(data_test), 400):\n","                        x_pred = np.zeros((1, params['maxlen']))\n","                        for t, char in enumerate(sentence):\n","                            x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n","                        preds = self.model.predict(x_pred, verbose=0)[0]\n","                        next_index = sample_pred(preds, diversity)\n","                        next_char = id2c[next_index]\n","                        if next_char == '<EOS>':\n","                            break\n","                        generated += [next_char]\n","                        sentence = sentence[1:] \n","                        sentence += [next_char]\n","                    print(''.join(generated))  # muestra por pantalla la frase generada\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-G4B_J-Mz0B","colab_type":"code","colab":{}},"source":["class HistoryDisplay(Callback):\n","# Muestra una gráfica de los accuracy y loss    \n","    def on_train_begin(self, logs={}):\n","        self.losses = []\n","        self.accs = []\n","        self.epochs = []\n","        self.fig, self.ax = plt.subplots()\n","        #plt.show()\n","        \n","        plt.ion()\n","        self.fig.show()\n","        self.fig.canvas.draw()\n","    \n","    def on_epoch_end(self, epoch, logs):\n","        self.epochs.append(epoch)\n","        self.losses.append(logs['loss'])\n","        self.accs.append(logs['acc'])\n","        if epoch % PLOT_EVERY == 0:\n","            \n","            self.ax.clear()\n","            self.ax.plot(self.epochs, self.accs, 'g', label='acc')\n","            self.ax.plot(self.epochs, self.losses, 'b', label='loss')\n","            legend = self.ax.legend(loc='upper right', shadow=True, fontsize='x-large')\n","            #display.clear_output(wait=True)\n","            #display.display(pl.gcf())\n","            self.fig.canvas.draw()\n","            \n","            #plt.draw()\n","        "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9HoaaKfMz0D","colab_type":"code","colab":{}},"source":["class TimeHistory(Callback):\n","# Devuelve lo que tarda cada epoch\n","    def on_train_begin(self, logs={}):\n","        self.times = []\n","\n","    def on_epoch_begin(self, batch, logs={}):\n","        self.epoch_time_start = time.time()\n","\n","    def on_epoch_end(self, batch, logs={}):\n","        self.times.append(time.time() - self.epoch_time_start)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3-j3k2CdHXeJ","colab_type":"text"},"source":["# Arquitectura y preparar el train y el predict"]},{"cell_type":"code","metadata":{"id":"NUbfuvv8Mz0K","colab_type":"code","colab":{}},"source":["class LM:\n","    def __init__(self, **kwargs):\n","        self.params = kwargs.pop('params', None)\n","        \n","    def compile_model(self, params={}):\n","        # capa de input \n","        lm_inputs = Input(shape=(params['maxlen'], ), name='lm_input')\n","        # para vincular palabras a vectores\n","        embedding = Embedding(params['vocab'], params['emb_feats'])(lm_inputs)\n","        # return sequences devuelve el resultado por cada w-emdedding por separado  \n","        lstm = CuDNNLSTM(params['rnn_hidden'], return_sequences=True, name='rnn1')\n","\n","        lmlstm = Bidirectional(lstm)(embedding)\n","        \n","        stacklstm = CuDNNLSTM(params['rnn_hidden'], return_sequences=False, name='stack')\n","        stackedlstm = stacklstm(lmlstm)\n","        #capa de salida\n","        out = Dense(params['vocab'], activation='softmax')(stackedlstm)\n","\n","        model = Model(lm_inputs, out)\n","        model.compile(\n","            optimizer='rmsprop',\n","            loss='categorical_crossentropy',\n","            metrics=['accuracy']\n","        )\n","        model.summary()\n","        return model\n","        \n","        \n","    def train(self, model, data, params={}):\n","        callbacks = self._get_callbacks()   #llamada a los callbacks de arriba\n","\n","        if 'shuffle' in params and params['shuffle']: #si la clave está en dicc de parámetros y es true\n","            shuffle(data)\n","\n","        sentences, next_chars = zip(*data)\n","        x = np.zeros((len(data), params['maxlen']))\n","        y = np.zeros((len(data), params['vocab']))\n","        for i, sentence in enumerate(sentences):\n","            for t, char in enumerate(sentence):\n","                x[i, t] = c2id[char] if char in c2id else c2id['<UNK>']\n","            y[i, c2id[next_chars[i]] if next_chars[i] in c2id else c2id['<UNK>']] = 1\n","\n","        model.fit(x, y, batch_size=params['batch_size'], epochs=params['epochs'], callbacks=callbacks, verbose=1)\n","        \n","    def predict(self, model, data, params={}):\n","        for diversity in [0.2, 0.6, 1.2]:\n","            print('------ diversity: ', diversity)\n","            sentence = copy(data)\n","            generated = copy(data)\n","            for i in range(len(data), 240): # si fuese un tweet\n","                x_pred = np.zeros((1, params['maxlen']))\n","                for t, char in enumerate(sentence):\n","                    x_pred[0, t] = c2id[char] if char in c2id else c2id['<UNK>']\n","                \n","                preds = self.model.predict(x_pred, verbose=0)[0]\n","                next_index = sample_pred(preds, diversity)\n","                next_char = id2c[next_index]\n","\n","                if next_char == '<EOS>':\n","                    break\n","                \n","                generated += [next_char]\n","                sentence = sentence[1:]\n","                sentence+= [next_char]\n","            \n","            print(''.join(generated))\n","        \n","    def _get_callbacks(self, model_path='model_lm.h5'):\n","     # monitoriza train y validation loss. Si en la 4ºepoch validation loss no baja, el train se detiene\n","        es = EarlyStopping(monitor='loss', patience=4, mode='auto', verbose=0)\n","     # cada vez que baja la validation loss guardará una copia del modelo. Machaca modelos peores. No guarda pesos.\n","        save_best = ModelCheckpoint(model_path, monitor='loss', verbose=0, save_best_only=True, save_weights_only=False, period=2)\n","     # reduce el learning rate si durante 3 epochs no ha bajado\n","        rlr = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose=1)\n","        st = Sampletest()\n","        hd = HistoryDisplay()\n","        return [st, rlr, es]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J1BvbwqhHe4j","colab_type":"text"},"source":["Hiperparametros y compilación."]},{"cell_type":"code","metadata":{"id":"Y8xxeun3Mz0N","colab_type":"code","colab":{}},"source":["compile_params = {\n","    'maxlen': maxlen,\n","    'vocab': len(vocab),\n","    'emb_feats': 100,\n","    'rnn_hidden': 256\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"adMpZ7bVMz0S","colab_type":"code","outputId":"17b2d308-a7c7-41ba-cb5f-d11d329892c2","executionInfo":{"status":"ok","timestamp":1582497960057,"user_tz":-60,"elapsed":36900,"user":{"displayName":"E MIN","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mBfimOqhMLNfQkhW92DtoyEHm90YQ8JHcfZWrSFRQ=s64","userId":"09902616180052958600"}},"colab":{"base_uri":"https://localhost:8080/","height":507}},"source":["lm = LM()\n","\n","lm_model = lm.compile_model(params=compile_params)"],"execution_count":21,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3576: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","Model: \"model_1\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","lm_input (InputLayer)        (None, 5)                 0         \n","_________________________________________________________________\n","embedding_1 (Embedding)      (None, 5, 100)            11100     \n","_________________________________________________________________\n","bidirectional_1 (Bidirection (None, 5, 512)            733184    \n","_________________________________________________________________\n","stack (CuDNNLSTM)            (None, 256)               788480    \n","_________________________________________________________________\n","dense_1 (Dense)              (None, 111)               28527     \n","=================================================================\n","Total params: 1,561,291\n","Trainable params: 1,561,291\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"XJmV8jqUHmJa","colab_type":"text"},"source":["Entrenar. Genera frases."]},{"cell_type":"code","metadata":{"id":"JXjlLxs9Mz0X","colab_type":"code","colab":{}},"source":["train_params = {\n","    'epochs': 500,\n","    'batch_size': 512,\n","    'shuffle': True,\n","    'vocab': len(vocab),\n","    'maxlen': maxlen,\n","}"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"jomgeNFgrKsL","colab_type":"code","outputId":"0c1d7b58-8bf0-4841-9bf9-4e40e4b58d0b","colab":{"base_uri":"https://localhost:8080/","height":759}},"source":["lm.train(lm_model, data=data_train, params=train_params)\n","# Genera frases como si las dijera Donald Trump (la frase empieza en SOS) cada 6 epochs"],"execution_count":0,"outputs":[{"output_type":"stream","text":["WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n","\n","Epoch 1/500\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n","\n","471229/471229 [==============================] - 28s 59us/step - loss: 2.4908 - acc: 0.3304\n","Epoch 2/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.8835 - acc: 0.4897\n","Epoch 3/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.6449 - acc: 0.5530\n","Epoch 4/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.5161 - acc: 0.5849\n","Epoch 5/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.4338 - acc: 0.6048\n","----- diversity: 0.2\n","<SOS>It was any with the was an and the Democrats and working in the U.S.A.....\n","----- diversity: 0.6\n","<SOS>It was as a truly was an encourage in their hard to stand the coming great the was an tremendous standing and and going very collusion which we are with the Fake News and continues to help on the Democrats and doing very well.\n","----- diversity: 1.2\n","<SOS>It was oFS oatics (Nations to dreamly going very underswSiem!\n","Epoch 6/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.3747 - acc: 0.6189\n","Epoch 7/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.3287 - acc: 0.6287\n","Epoch 8/500\n","471229/471229 [==============================] - 11s 24us/step - loss: 1.2907 - acc: 0.6369\n","Epoch 9/500\n","214016/471229 [============>.................] - ETA: 6s - loss: 1.2509 - acc: 0.6460"],"name":"stdout"}]}]}